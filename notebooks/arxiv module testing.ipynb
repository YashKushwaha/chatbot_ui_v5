{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e416a0d-a27d-4aa2-bfa1-a7b9fa28d008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47edc27e-25a0-478f-9721-cf32fbac932f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for papers\n",
    "search = arxiv.Search(\n",
    "    query=\"multimodal rag\",\n",
    "    max_results=3,\n",
    "    sort_by=arxiv.SortCriterion.SubmittedDate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa603fdb-5fdd-4eda-941a-f2c2bd2328ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6647/264740563.py:1: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Impact of Pretraining Word Co-occurrence on Compositional Generalization in Multimodal Models\n",
      "PDF URL: http://arxiv.org/pdf/2507.08000v1\n",
      "Title: Multigranular Evaluation for Brain Visual Decoding\n",
      "PDF URL: http://arxiv.org/pdf/2507.07993v1\n",
      "Title: OST-Bench: Evaluating the Capabilities of MLLMs in Online Spatio-temporal Scene Understanding\n",
      "PDF URL: http://arxiv.org/pdf/2507.07984v1\n"
     ]
    }
   ],
   "source": [
    "for result in search.results():\n",
    "    print(f\"Title: {result.title}\")\n",
    "    print(f\"PDF URL: {result.pdf_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e31ec2-284a-4e03-81a0-f14f4bf93df3",
   "metadata": {},
   "source": [
    "#### Since search.results is deprecated let's use the Client.results method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "191e76d8-daed-4d24-a580-94e96ccc9c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Impact of Pretraining Word Co-occurrence on Compositional Generalization in Multimodal Models\n",
      "PDF URL: http://arxiv.org/pdf/2507.08000v1\n",
      "-----\n",
      "Multigranular Evaluation for Brain Visual Decoding\n",
      "PDF URL: http://arxiv.org/pdf/2507.07993v1\n",
      "-----\n",
      "OST-Bench: Evaluating the Capabilities of MLLMs in Online Spatio-temporal Scene Understanding\n",
      "PDF URL: http://arxiv.org/pdf/2507.07984v1\n",
      "-----\n",
      "Performance and Practical Considerations of Large and Small Language Models in Clinical Decision Support in Rheumatology\n",
      "PDF URL: http://arxiv.org/pdf/2507.07983v1\n",
      "-----\n",
      "Martian World Models: Controllable Video Synthesis with Physically Accurate 3D Reconstructions\n",
      "PDF URL: http://arxiv.org/pdf/2507.07978v1\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "client = arxiv.Client()\n",
    "\n",
    "search = arxiv.Search(\n",
    "    query=\"multimodal rag\",\n",
    "    max_results=5,\n",
    "    sort_by=arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "\n",
    "for result in client.results(search):\n",
    "    print(result.title)\n",
    "    print(f\"PDF URL: {result.pdf_url}\")\n",
    "    print(5*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcd83569-b79f-42de-8e64-aa65c7e7c451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry_id\n",
      "-----\n",
      "http://arxiv.org/abs/2507.08000v1\n",
      "updated\n",
      "-----\n",
      "2025-07-10 17:59:59+00:00\n",
      "published\n",
      "-----\n",
      "2025-07-10 17:59:59+00:00\n",
      "title\n",
      "-----\n",
      "Impact of Pretraining Word Co-occurrence on Compositional Generalization in Multimodal Models\n",
      "authors\n",
      "-----\n",
      "[arxiv.Result.Author('Helen Qu'), arxiv.Result.Author('Sang Michael Xie')]\n",
      "summary\n",
      "-----\n",
      "CLIP and large multimodal models (LMMs) have better accuracy on examples\n",
      "involving concepts that are highly represented in the training data. However,\n",
      "the role of concept combinations in the training data on compositional\n",
      "generalization is largely unclear -- for instance, how does accuracy vary when\n",
      "a common object appears in an uncommon pairing with another object? In this\n",
      "paper, we investigate how word co-occurrence statistics in the pretraining\n",
      "dataset (a proxy for co-occurrence of visual concepts) impacts CLIP/LMM\n",
      "performance. To disentangle the effects of word co-occurrence frequencies from\n",
      "single-word frequencies, we measure co-occurrence with pointwise mutual\n",
      "information (PMI), which normalizes the joint probability of two words\n",
      "co-occurring by the probability of co-occurring independently. Using\n",
      "synthetically generated images with a variety of concept pairs, we show a\n",
      "strong correlation between PMI in the CLIP pretraining data and zero-shot\n",
      "accuracy in CLIP models trained on LAION-400M (r=0.97 and 14% accuracy gap\n",
      "between images in the top and bottom 5% of PMI values), demonstrating that even\n",
      "accuracy on common concepts is affected by the combination of concepts in the\n",
      "image. Leveraging this finding, we reproduce this effect in natural images by\n",
      "editing them to contain pairs with varying PMI, resulting in a correlation of\n",
      "r=0.75. Finally, we demonstrate that this behavior in CLIP transfers to LMMs\n",
      "built on top of CLIP (r=0.70 for TextVQA, r=0.62 for VQAv2). Our findings\n",
      "highlight the need for algorithms and architectures that improve compositional\n",
      "generalization in multimodal models without scaling the training data\n",
      "combinatorially. Our code is available at\n",
      "https://github.com/helenqu/multimodal-pretraining-pmi.\n",
      "comment\n",
      "-----\n",
      "None\n",
      "journal_ref\n",
      "-----\n",
      "None\n",
      "doi\n",
      "-----\n",
      "None\n",
      "primary_category\n",
      "-----\n",
      "cs.CV\n",
      "categories\n",
      "-----\n",
      "['cs.CV', 'cs.LG']\n",
      "links\n",
      "-----\n",
      "[arxiv.Result.Link('http://arxiv.org/abs/2507.08000v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2507.08000v1', title='pdf', rel='related', content_type=None)]\n",
      "pdf_url\n",
      "-----\n",
      "http://arxiv.org/pdf/2507.08000v1\n",
      "_raw\n",
      "-----\n",
      "{'id': 'http://arxiv.org/abs/2507.08000v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2507.08000v1', 'updated': '2025-07-10T17:59:59Z', 'updated_parsed': time.struct_time(tm_year=2025, tm_mon=7, tm_mday=10, tm_hour=17, tm_min=59, tm_sec=59, tm_wday=3, tm_yday=191, tm_isdst=0), 'published': '2025-07-10T17:59:59Z', 'published_parsed': time.struct_time(tm_year=2025, tm_mon=7, tm_mday=10, tm_hour=17, tm_min=59, tm_sec=59, tm_wday=3, tm_yday=191, tm_isdst=0), 'title': 'Impact of Pretraining Word Co-occurrence on Compositional Generalization\\n  in Multimodal Models', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Impact of Pretraining Word Co-occurrence on Compositional Generalization\\n  in Multimodal Models'}, 'summary': 'CLIP and large multimodal models (LMMs) have better accuracy on examples\\ninvolving concepts that are highly represented in the training data. However,\\nthe role of concept combinations in the training data on compositional\\ngeneralization is largely unclear -- for instance, how does accuracy vary when\\na common object appears in an uncommon pairing with another object? In this\\npaper, we investigate how word co-occurrence statistics in the pretraining\\ndataset (a proxy for co-occurrence of visual concepts) impacts CLIP/LMM\\nperformance. To disentangle the effects of word co-occurrence frequencies from\\nsingle-word frequencies, we measure co-occurrence with pointwise mutual\\ninformation (PMI), which normalizes the joint probability of two words\\nco-occurring by the probability of co-occurring independently. Using\\nsynthetically generated images with a variety of concept pairs, we show a\\nstrong correlation between PMI in the CLIP pretraining data and zero-shot\\naccuracy in CLIP models trained on LAION-400M (r=0.97 and 14% accuracy gap\\nbetween images in the top and bottom 5% of PMI values), demonstrating that even\\naccuracy on common concepts is affected by the combination of concepts in the\\nimage. Leveraging this finding, we reproduce this effect in natural images by\\nediting them to contain pairs with varying PMI, resulting in a correlation of\\nr=0.75. Finally, we demonstrate that this behavior in CLIP transfers to LMMs\\nbuilt on top of CLIP (r=0.70 for TextVQA, r=0.62 for VQAv2). Our findings\\nhighlight the need for algorithms and architectures that improve compositional\\ngeneralization in multimodal models without scaling the training data\\ncombinatorially. Our code is available at\\nhttps://github.com/helenqu/multimodal-pretraining-pmi.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'CLIP and large multimodal models (LMMs) have better accuracy on examples\\ninvolving concepts that are highly represented in the training data. However,\\nthe role of concept combinations in the training data on compositional\\ngeneralization is largely unclear -- for instance, how does accuracy vary when\\na common object appears in an uncommon pairing with another object? In this\\npaper, we investigate how word co-occurrence statistics in the pretraining\\ndataset (a proxy for co-occurrence of visual concepts) impacts CLIP/LMM\\nperformance. To disentangle the effects of word co-occurrence frequencies from\\nsingle-word frequencies, we measure co-occurrence with pointwise mutual\\ninformation (PMI), which normalizes the joint probability of two words\\nco-occurring by the probability of co-occurring independently. Using\\nsynthetically generated images with a variety of concept pairs, we show a\\nstrong correlation between PMI in the CLIP pretraining data and zero-shot\\naccuracy in CLIP models trained on LAION-400M (r=0.97 and 14% accuracy gap\\nbetween images in the top and bottom 5% of PMI values), demonstrating that even\\naccuracy on common concepts is affected by the combination of concepts in the\\nimage. Leveraging this finding, we reproduce this effect in natural images by\\nediting them to contain pairs with varying PMI, resulting in a correlation of\\nr=0.75. Finally, we demonstrate that this behavior in CLIP transfers to LMMs\\nbuilt on top of CLIP (r=0.70 for TextVQA, r=0.62 for VQAv2). Our findings\\nhighlight the need for algorithms and architectures that improve compositional\\ngeneralization in multimodal models without scaling the training data\\ncombinatorially. Our code is available at\\nhttps://github.com/helenqu/multimodal-pretraining-pmi.'}, 'authors': [{'name': 'Helen Qu'}, {'name': 'Sang Michael Xie'}], 'author_detail': {'name': 'Sang Michael Xie'}, 'author': 'Sang Michael Xie', 'links': [{'href': 'http://arxiv.org/abs/2507.08000v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2507.08000v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}\n",
      "=====\n",
      "entry_id\n",
      "-----\n",
      "http://arxiv.org/abs/2507.07993v1\n",
      "updated\n",
      "-----\n",
      "2025-07-10 17:59:24+00:00\n",
      "published\n",
      "-----\n",
      "2025-07-10 17:59:24+00:00\n",
      "title\n",
      "-----\n",
      "Multigranular Evaluation for Brain Visual Decoding\n",
      "authors\n",
      "-----\n",
      "[arxiv.Result.Author('Weihao Xia'), arxiv.Result.Author('Cengiz Oztireli')]\n",
      "summary\n",
      "-----\n",
      "Existing evaluation protocols for brain visual decoding predominantly rely on\n",
      "coarse metrics that obscure inter-model differences, lack neuroscientific\n",
      "foundation, and fail to capture fine-grained visual distinctions. To address\n",
      "these limitations, we introduce BASIC, a unified, multigranular evaluation\n",
      "framework that jointly quantifies structural fidelity, inferential alignment,\n",
      "and contextual coherence between decoded and ground truth images. For the\n",
      "structural level, we introduce a hierarchical suite of segmentation-based\n",
      "metrics, including foreground, semantic, instance, and component masks,\n",
      "anchored in granularity-aware correspondence across mask structures. For the\n",
      "semantic level, we extract structured scene representations encompassing\n",
      "objects, attributes, and relationships using multimodal large language models,\n",
      "enabling detailed, scalable, and context-rich comparisons with ground-truth\n",
      "stimuli. We benchmark a diverse set of visual decoding methods across multiple\n",
      "stimulus-neuroimaging datasets within this unified evaluation framework.\n",
      "Together, these criteria provide a more discriminative, interpretable, and\n",
      "comprehensive foundation for measuring brain visual decoding methods.\n",
      "comment\n",
      "-----\n",
      "Project: https://weihaox.github.io/BASIC\n",
      "journal_ref\n",
      "-----\n",
      "None\n",
      "doi\n",
      "-----\n",
      "None\n",
      "primary_category\n",
      "-----\n",
      "cs.CV\n",
      "categories\n",
      "-----\n",
      "['cs.CV', 'cs.AI', 'eess.IV', 'q-bio.NC']\n",
      "links\n",
      "-----\n",
      "[arxiv.Result.Link('http://arxiv.org/abs/2507.07993v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2507.07993v1', title='pdf', rel='related', content_type=None)]\n",
      "pdf_url\n",
      "-----\n",
      "http://arxiv.org/pdf/2507.07993v1\n",
      "_raw\n",
      "-----\n",
      "{'id': 'http://arxiv.org/abs/2507.07993v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2507.07993v1', 'updated': '2025-07-10T17:59:24Z', 'updated_parsed': time.struct_time(tm_year=2025, tm_mon=7, tm_mday=10, tm_hour=17, tm_min=59, tm_sec=24, tm_wday=3, tm_yday=191, tm_isdst=0), 'published': '2025-07-10T17:59:24Z', 'published_parsed': time.struct_time(tm_year=2025, tm_mon=7, tm_mday=10, tm_hour=17, tm_min=59, tm_sec=24, tm_wday=3, tm_yday=191, tm_isdst=0), 'title': 'Multigranular Evaluation for Brain Visual Decoding', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Multigranular Evaluation for Brain Visual Decoding'}, 'summary': 'Existing evaluation protocols for brain visual decoding predominantly rely on\\ncoarse metrics that obscure inter-model differences, lack neuroscientific\\nfoundation, and fail to capture fine-grained visual distinctions. To address\\nthese limitations, we introduce BASIC, a unified, multigranular evaluation\\nframework that jointly quantifies structural fidelity, inferential alignment,\\nand contextual coherence between decoded and ground truth images. For the\\nstructural level, we introduce a hierarchical suite of segmentation-based\\nmetrics, including foreground, semantic, instance, and component masks,\\nanchored in granularity-aware correspondence across mask structures. For the\\nsemantic level, we extract structured scene representations encompassing\\nobjects, attributes, and relationships using multimodal large language models,\\nenabling detailed, scalable, and context-rich comparisons with ground-truth\\nstimuli. We benchmark a diverse set of visual decoding methods across multiple\\nstimulus-neuroimaging datasets within this unified evaluation framework.\\nTogether, these criteria provide a more discriminative, interpretable, and\\ncomprehensive foundation for measuring brain visual decoding methods.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Existing evaluation protocols for brain visual decoding predominantly rely on\\ncoarse metrics that obscure inter-model differences, lack neuroscientific\\nfoundation, and fail to capture fine-grained visual distinctions. To address\\nthese limitations, we introduce BASIC, a unified, multigranular evaluation\\nframework that jointly quantifies structural fidelity, inferential alignment,\\nand contextual coherence between decoded and ground truth images. For the\\nstructural level, we introduce a hierarchical suite of segmentation-based\\nmetrics, including foreground, semantic, instance, and component masks,\\nanchored in granularity-aware correspondence across mask structures. For the\\nsemantic level, we extract structured scene representations encompassing\\nobjects, attributes, and relationships using multimodal large language models,\\nenabling detailed, scalable, and context-rich comparisons with ground-truth\\nstimuli. We benchmark a diverse set of visual decoding methods across multiple\\nstimulus-neuroimaging datasets within this unified evaluation framework.\\nTogether, these criteria provide a more discriminative, interpretable, and\\ncomprehensive foundation for measuring brain visual decoding methods.'}, 'authors': [{'name': 'Weihao Xia'}, {'name': 'Cengiz Oztireli'}], 'author_detail': {'name': 'Cengiz Oztireli'}, 'author': 'Cengiz Oztireli', 'arxiv_comment': 'Project: https://weihaox.github.io/BASIC', 'links': [{'href': 'http://arxiv.org/abs/2507.07993v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2507.07993v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.IV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'q-bio.NC', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}\n",
      "=====\n",
      "entry_id\n",
      "-----\n",
      "http://arxiv.org/abs/2507.07984v1\n",
      "updated\n",
      "-----\n",
      "2025-07-10 17:56:07+00:00\n",
      "published\n",
      "-----\n",
      "2025-07-10 17:56:07+00:00\n",
      "title\n",
      "-----\n",
      "OST-Bench: Evaluating the Capabilities of MLLMs in Online Spatio-temporal Scene Understanding\n",
      "authors\n",
      "-----\n",
      "[arxiv.Result.Author('JingLi Lin'), arxiv.Result.Author('Chenming Zhu'), arxiv.Result.Author('Runsen Xu'), arxiv.Result.Author('Xiaohan Mao'), arxiv.Result.Author('Xihui Liu'), arxiv.Result.Author('Tai Wang'), arxiv.Result.Author('Jiangmiao Pang')]\n",
      "summary\n",
      "-----\n",
      "Recent advances in multimodal large language models (MLLMs) have shown\n",
      "remarkable capabilities in integrating vision and language for complex\n",
      "reasoning. While most existing benchmarks evaluate models under offline\n",
      "settings with a fixed set of pre-recorded inputs, we introduce OST-Bench, a\n",
      "benchmark designed to evaluate Online Spatio-Temporal understanding from the\n",
      "perspective of an agent actively exploring a scene. The Online aspect\n",
      "emphasizes the need to process and reason over incrementally acquired\n",
      "observations, while the Spatio-Temporal component requires integrating current\n",
      "visual inputs with historical memory to support dynamic spatial reasoning.\n",
      "OST-Bench better reflects the challenges of real-world embodied perception.\n",
      "Built on an efficient data collection pipeline, OST-Bench consists of 1.4k\n",
      "scenes and 10k question-answer pairs collected from ScanNet, Matterport3D, and\n",
      "ARKitScenes. We evaluate several leading MLLMs on OST-Bench and observe that\n",
      "they fall short on tasks requiring complex spatio-temporal reasoning. Under the\n",
      "online setting, their accuracy declines as the exploration horizon extends and\n",
      "the memory grows. Through further experimental analysis, we identify common\n",
      "error patterns across models and find that both complex clue-based spatial\n",
      "reasoning demands and long-term memory retrieval requirements significantly\n",
      "drop model performance along two separate axes, highlighting the core\n",
      "challenges that must be addressed to improve online embodied reasoning. To\n",
      "foster further research and development in the field, our codes, dataset, and\n",
      "benchmark are available. Our project page is:\n",
      "https://rbler1234.github.io/OSTBench.github.io/\n",
      "comment\n",
      "-----\n",
      "28 pages, a benchmark designed to evaluate Online Spatio-Temporal\n",
      "  understanding from the perspective of an agent actively exploring a scene.\n",
      "  Project Page: https://rbler1234.github.io/OSTBench.github.io/\n",
      "journal_ref\n",
      "-----\n",
      "None\n",
      "doi\n",
      "-----\n",
      "None\n",
      "primary_category\n",
      "-----\n",
      "cs.CV\n",
      "categories\n",
      "-----\n",
      "['cs.CV']\n",
      "links\n",
      "-----\n",
      "[arxiv.Result.Link('http://arxiv.org/abs/2507.07984v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2507.07984v1', title='pdf', rel='related', content_type=None)]\n",
      "pdf_url\n",
      "-----\n",
      "http://arxiv.org/pdf/2507.07984v1\n",
      "_raw\n",
      "-----\n",
      "{'id': 'http://arxiv.org/abs/2507.07984v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2507.07984v1', 'updated': '2025-07-10T17:56:07Z', 'updated_parsed': time.struct_time(tm_year=2025, tm_mon=7, tm_mday=10, tm_hour=17, tm_min=56, tm_sec=7, tm_wday=3, tm_yday=191, tm_isdst=0), 'published': '2025-07-10T17:56:07Z', 'published_parsed': time.struct_time(tm_year=2025, tm_mon=7, tm_mday=10, tm_hour=17, tm_min=56, tm_sec=7, tm_wday=3, tm_yday=191, tm_isdst=0), 'title': 'OST-Bench: Evaluating the Capabilities of MLLMs in Online\\n  Spatio-temporal Scene Understanding', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'OST-Bench: Evaluating the Capabilities of MLLMs in Online\\n  Spatio-temporal Scene Understanding'}, 'summary': 'Recent advances in multimodal large language models (MLLMs) have shown\\nremarkable capabilities in integrating vision and language for complex\\nreasoning. While most existing benchmarks evaluate models under offline\\nsettings with a fixed set of pre-recorded inputs, we introduce OST-Bench, a\\nbenchmark designed to evaluate Online Spatio-Temporal understanding from the\\nperspective of an agent actively exploring a scene. The Online aspect\\nemphasizes the need to process and reason over incrementally acquired\\nobservations, while the Spatio-Temporal component requires integrating current\\nvisual inputs with historical memory to support dynamic spatial reasoning.\\nOST-Bench better reflects the challenges of real-world embodied perception.\\nBuilt on an efficient data collection pipeline, OST-Bench consists of 1.4k\\nscenes and 10k question-answer pairs collected from ScanNet, Matterport3D, and\\nARKitScenes. We evaluate several leading MLLMs on OST-Bench and observe that\\nthey fall short on tasks requiring complex spatio-temporal reasoning. Under the\\nonline setting, their accuracy declines as the exploration horizon extends and\\nthe memory grows. Through further experimental analysis, we identify common\\nerror patterns across models and find that both complex clue-based spatial\\nreasoning demands and long-term memory retrieval requirements significantly\\ndrop model performance along two separate axes, highlighting the core\\nchallenges that must be addressed to improve online embodied reasoning. To\\nfoster further research and development in the field, our codes, dataset, and\\nbenchmark are available. Our project page is:\\nhttps://rbler1234.github.io/OSTBench.github.io/', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Recent advances in multimodal large language models (MLLMs) have shown\\nremarkable capabilities in integrating vision and language for complex\\nreasoning. While most existing benchmarks evaluate models under offline\\nsettings with a fixed set of pre-recorded inputs, we introduce OST-Bench, a\\nbenchmark designed to evaluate Online Spatio-Temporal understanding from the\\nperspective of an agent actively exploring a scene. The Online aspect\\nemphasizes the need to process and reason over incrementally acquired\\nobservations, while the Spatio-Temporal component requires integrating current\\nvisual inputs with historical memory to support dynamic spatial reasoning.\\nOST-Bench better reflects the challenges of real-world embodied perception.\\nBuilt on an efficient data collection pipeline, OST-Bench consists of 1.4k\\nscenes and 10k question-answer pairs collected from ScanNet, Matterport3D, and\\nARKitScenes. We evaluate several leading MLLMs on OST-Bench and observe that\\nthey fall short on tasks requiring complex spatio-temporal reasoning. Under the\\nonline setting, their accuracy declines as the exploration horizon extends and\\nthe memory grows. Through further experimental analysis, we identify common\\nerror patterns across models and find that both complex clue-based spatial\\nreasoning demands and long-term memory retrieval requirements significantly\\ndrop model performance along two separate axes, highlighting the core\\nchallenges that must be addressed to improve online embodied reasoning. To\\nfoster further research and development in the field, our codes, dataset, and\\nbenchmark are available. Our project page is:\\nhttps://rbler1234.github.io/OSTBench.github.io/'}, 'authors': [{'name': 'JingLi Lin'}, {'name': 'Chenming Zhu'}, {'name': 'Runsen Xu'}, {'name': 'Xiaohan Mao'}, {'name': 'Xihui Liu'}, {'name': 'Tai Wang'}, {'name': 'Jiangmiao Pang'}], 'author_detail': {'name': 'Jiangmiao Pang'}, 'author': 'Jiangmiao Pang', 'arxiv_comment': '28 pages, a benchmark designed to evaluate Online Spatio-Temporal\\n  understanding from the perspective of an agent actively exploring a scene.\\n  Project Page: https://rbler1234.github.io/OSTBench.github.io/', 'links': [{'href': 'http://arxiv.org/abs/2507.07984v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2507.07984v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}\n",
      "=====\n",
      "entry_id\n",
      "-----\n",
      "http://arxiv.org/abs/2507.07983v1\n",
      "updated\n",
      "-----\n",
      "2025-07-10 17:56:03+00:00\n",
      "published\n",
      "-----\n",
      "2025-07-10 17:56:03+00:00\n",
      "title\n",
      "-----\n",
      "Performance and Practical Considerations of Large and Small Language Models in Clinical Decision Support in Rheumatology\n",
      "authors\n",
      "-----\n",
      "[arxiv.Result.Author('Sabine Felde'), arxiv.Result.Author('Rüdiger Buchkremer'), arxiv.Result.Author('Gamal Chehab'), arxiv.Result.Author('Christian Thielscher'), arxiv.Result.Author('Jörg HW Distler'), arxiv.Result.Author('Matthias Schneider'), arxiv.Result.Author('Jutta G. Richter')]\n",
      "summary\n",
      "-----\n",
      "Large language models (LLMs) show promise for supporting clinical\n",
      "decision-making in complex fields such as rheumatology. Our evaluation shows\n",
      "that smaller language models (SLMs), combined with retrieval-augmented\n",
      "generation (RAG), achieve higher diagnostic and therapeutic performance than\n",
      "larger models, while requiring substantially less energy and enabling\n",
      "cost-efficient, local deployment. These features are attractive for\n",
      "resource-limited healthcare. However, expert oversight remains essential, as no\n",
      "model consistently reached specialist-level accuracy in rheumatology.\n",
      "comment\n",
      "-----\n",
      "None\n",
      "journal_ref\n",
      "-----\n",
      "None\n",
      "doi\n",
      "-----\n",
      "None\n",
      "primary_category\n",
      "-----\n",
      "cs.CL\n",
      "categories\n",
      "-----\n",
      "['cs.CL', 'cs.AI', 'L01.224.900.500 (Primary), L01.700.508.300, L01.224.050.375,\\n  H02.403.720.750, N04.590, N04.452.758.625 (Secondary)', 'I.2.7; H.3.3; J.3; I.2.9; C.4']\n",
      "links\n",
      "-----\n",
      "[arxiv.Result.Link('http://arxiv.org/abs/2507.07983v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2507.07983v1', title='pdf', rel='related', content_type=None)]\n",
      "pdf_url\n",
      "-----\n",
      "http://arxiv.org/pdf/2507.07983v1\n",
      "_raw\n",
      "-----\n",
      "{'id': 'http://arxiv.org/abs/2507.07983v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2507.07983v1', 'updated': '2025-07-10T17:56:03Z', 'updated_parsed': time.struct_time(tm_year=2025, tm_mon=7, tm_mday=10, tm_hour=17, tm_min=56, tm_sec=3, tm_wday=3, tm_yday=191, tm_isdst=0), 'published': '2025-07-10T17:56:03Z', 'published_parsed': time.struct_time(tm_year=2025, tm_mon=7, tm_mday=10, tm_hour=17, tm_min=56, tm_sec=3, tm_wday=3, tm_yday=191, tm_isdst=0), 'title': 'Performance and Practical Considerations of Large and Small Language\\n  Models in Clinical Decision Support in Rheumatology', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Performance and Practical Considerations of Large and Small Language\\n  Models in Clinical Decision Support in Rheumatology'}, 'summary': 'Large language models (LLMs) show promise for supporting clinical\\ndecision-making in complex fields such as rheumatology. Our evaluation shows\\nthat smaller language models (SLMs), combined with retrieval-augmented\\ngeneration (RAG), achieve higher diagnostic and therapeutic performance than\\nlarger models, while requiring substantially less energy and enabling\\ncost-efficient, local deployment. These features are attractive for\\nresource-limited healthcare. However, expert oversight remains essential, as no\\nmodel consistently reached specialist-level accuracy in rheumatology.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Large language models (LLMs) show promise for supporting clinical\\ndecision-making in complex fields such as rheumatology. Our evaluation shows\\nthat smaller language models (SLMs), combined with retrieval-augmented\\ngeneration (RAG), achieve higher diagnostic and therapeutic performance than\\nlarger models, while requiring substantially less energy and enabling\\ncost-efficient, local deployment. These features are attractive for\\nresource-limited healthcare. However, expert oversight remains essential, as no\\nmodel consistently reached specialist-level accuracy in rheumatology.'}, 'authors': [{'name': 'Sabine Felde'}, {'name': 'Rüdiger Buchkremer'}, {'name': 'Gamal Chehab'}, {'name': 'Christian Thielscher'}, {'name': 'Jörg HW Distler'}, {'name': 'Matthias Schneider'}, {'name': 'Jutta G. Richter'}], 'author_detail': {'name': 'Jutta G. Richter'}, 'author': 'Jutta G. Richter', 'links': [{'href': 'http://arxiv.org/abs/2507.07983v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2507.07983v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'L01.224.900.500 (Primary), L01.700.508.300, L01.224.050.375,\\n  H02.403.720.750, N04.590, N04.452.758.625 (Secondary)', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'I.2.7; H.3.3; J.3; I.2.9; C.4', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}\n",
      "=====\n",
      "entry_id\n",
      "-----\n",
      "http://arxiv.org/abs/2507.07978v1\n",
      "updated\n",
      "-----\n",
      "2025-07-10 17:54:27+00:00\n",
      "published\n",
      "-----\n",
      "2025-07-10 17:54:27+00:00\n",
      "title\n",
      "-----\n",
      "Martian World Models: Controllable Video Synthesis with Physically Accurate 3D Reconstructions\n",
      "authors\n",
      "-----\n",
      "[arxiv.Result.Author('Longfei Li'), arxiv.Result.Author('Zhiwen Fan'), arxiv.Result.Author('Wenyan Cong'), arxiv.Result.Author('Xinhang Liu'), arxiv.Result.Author('Yuyang Yin'), arxiv.Result.Author('Matt Foutter'), arxiv.Result.Author('Panwang Pan'), arxiv.Result.Author('Chenyu You'), arxiv.Result.Author('Yue Wang'), arxiv.Result.Author('Zhangyang Wang'), arxiv.Result.Author('Yao Zhao'), arxiv.Result.Author('Marco Pavone'), arxiv.Result.Author('Yunchao Wei')]\n",
      "summary\n",
      "-----\n",
      "Synthesizing realistic Martian landscape videos is crucial for mission\n",
      "rehearsal and robotic simulation. However, this task poses unique challenges\n",
      "due to the scarcity of high-quality Martian data and the significant domain gap\n",
      "between Martian and terrestrial imagery. To address these challenges, we\n",
      "propose a holistic solution composed of two key components: 1) A data curation\n",
      "pipeline Multimodal Mars Synthesis (M3arsSynth), which reconstructs 3D Martian\n",
      "environments from real stereo navigation images, sourced from NASA's Planetary\n",
      "Data System (PDS), and renders high-fidelity multiview 3D video sequences. 2) A\n",
      "Martian terrain video generator, MarsGen, which synthesizes novel videos\n",
      "visually realistic and geometrically consistent with the 3D structure encoded\n",
      "in the data. Our M3arsSynth engine spans a wide range of Martian terrains and\n",
      "acquisition dates, enabling the generation of physically accurate 3D surface\n",
      "models at metric-scale resolution. MarsGen, fine-tuned on M3arsSynth data,\n",
      "synthesizes videos conditioned on an initial image frame and, optionally,\n",
      "camera trajectories or textual prompts, allowing for video generation in novel\n",
      "environments. Experimental results show that our approach outperforms video\n",
      "synthesis models trained on terrestrial datasets, achieving superior visual\n",
      "fidelity and 3D structural consistency.\n",
      "comment\n",
      "-----\n",
      "Project Page: https://marsgenai.github.io\n",
      "journal_ref\n",
      "-----\n",
      "None\n",
      "doi\n",
      "-----\n",
      "None\n",
      "primary_category\n",
      "-----\n",
      "cs.CV\n",
      "categories\n",
      "-----\n",
      "['cs.CV']\n",
      "links\n",
      "-----\n",
      "[arxiv.Result.Link('http://arxiv.org/abs/2507.07978v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2507.07978v1', title='pdf', rel='related', content_type=None)]\n",
      "pdf_url\n",
      "-----\n",
      "http://arxiv.org/pdf/2507.07978v1\n",
      "_raw\n",
      "-----\n",
      "{'id': 'http://arxiv.org/abs/2507.07978v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2507.07978v1', 'updated': '2025-07-10T17:54:27Z', 'updated_parsed': time.struct_time(tm_year=2025, tm_mon=7, tm_mday=10, tm_hour=17, tm_min=54, tm_sec=27, tm_wday=3, tm_yday=191, tm_isdst=0), 'published': '2025-07-10T17:54:27Z', 'published_parsed': time.struct_time(tm_year=2025, tm_mon=7, tm_mday=10, tm_hour=17, tm_min=54, tm_sec=27, tm_wday=3, tm_yday=191, tm_isdst=0), 'title': 'Martian World Models: Controllable Video Synthesis with Physically\\n  Accurate 3D Reconstructions', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Martian World Models: Controllable Video Synthesis with Physically\\n  Accurate 3D Reconstructions'}, 'summary': \"Synthesizing realistic Martian landscape videos is crucial for mission\\nrehearsal and robotic simulation. However, this task poses unique challenges\\ndue to the scarcity of high-quality Martian data and the significant domain gap\\nbetween Martian and terrestrial imagery. To address these challenges, we\\npropose a holistic solution composed of two key components: 1) A data curation\\npipeline Multimodal Mars Synthesis (M3arsSynth), which reconstructs 3D Martian\\nenvironments from real stereo navigation images, sourced from NASA's Planetary\\nData System (PDS), and renders high-fidelity multiview 3D video sequences. 2) A\\nMartian terrain video generator, MarsGen, which synthesizes novel videos\\nvisually realistic and geometrically consistent with the 3D structure encoded\\nin the data. Our M3arsSynth engine spans a wide range of Martian terrains and\\nacquisition dates, enabling the generation of physically accurate 3D surface\\nmodels at metric-scale resolution. MarsGen, fine-tuned on M3arsSynth data,\\nsynthesizes videos conditioned on an initial image frame and, optionally,\\ncamera trajectories or textual prompts, allowing for video generation in novel\\nenvironments. Experimental results show that our approach outperforms video\\nsynthesis models trained on terrestrial datasets, achieving superior visual\\nfidelity and 3D structural consistency.\", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': \"Synthesizing realistic Martian landscape videos is crucial for mission\\nrehearsal and robotic simulation. However, this task poses unique challenges\\ndue to the scarcity of high-quality Martian data and the significant domain gap\\nbetween Martian and terrestrial imagery. To address these challenges, we\\npropose a holistic solution composed of two key components: 1) A data curation\\npipeline Multimodal Mars Synthesis (M3arsSynth), which reconstructs 3D Martian\\nenvironments from real stereo navigation images, sourced from NASA's Planetary\\nData System (PDS), and renders high-fidelity multiview 3D video sequences. 2) A\\nMartian terrain video generator, MarsGen, which synthesizes novel videos\\nvisually realistic and geometrically consistent with the 3D structure encoded\\nin the data. Our M3arsSynth engine spans a wide range of Martian terrains and\\nacquisition dates, enabling the generation of physically accurate 3D surface\\nmodels at metric-scale resolution. MarsGen, fine-tuned on M3arsSynth data,\\nsynthesizes videos conditioned on an initial image frame and, optionally,\\ncamera trajectories or textual prompts, allowing for video generation in novel\\nenvironments. Experimental results show that our approach outperforms video\\nsynthesis models trained on terrestrial datasets, achieving superior visual\\nfidelity and 3D structural consistency.\"}, 'authors': [{'name': 'Longfei Li'}, {'name': 'Zhiwen Fan'}, {'name': 'Wenyan Cong'}, {'name': 'Xinhang Liu'}, {'name': 'Yuyang Yin'}, {'name': 'Matt Foutter'}, {'name': 'Panwang Pan'}, {'name': 'Chenyu You'}, {'name': 'Yue Wang'}, {'name': 'Zhangyang Wang'}, {'name': 'Yao Zhao'}, {'name': 'Marco Pavone'}, {'name': 'Yunchao Wei'}], 'author_detail': {'name': 'Yunchao Wei'}, 'author': 'Yunchao Wei', 'arxiv_comment': 'Project Page: https://marsgenai.github.io', 'links': [{'href': 'http://arxiv.org/abs/2507.07978v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2507.07978v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}\n",
      "=====\n"
     ]
    }
   ],
   "source": [
    "for result in client.results(search):\n",
    "    for i,j in result.__dict__.items():\n",
    "        print(i)\n",
    "        print(5*'-')\n",
    "        print(j)\n",
    "\n",
    "    print(5*'=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "761d7e85-2322-4618-90dc-7258d7322bb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Client' object has no attribute 'Search'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m help(\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSearch\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Client' object has no attribute 'Search'"
     ]
    }
   ],
   "source": [
    "help(client.Search)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
